{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Vishwesh\\Anaconda3\\envs\\deep_l\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat, savemat\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from keras.optimizers import SGD, adam, nadam, Adagrad\n",
    "from keras.regularizers import l1,l2\n",
    "\n",
    "from keras.callbacks import EarlyStopping, CSVLogger\n",
    "from keras.losses import mean_squared_logarithmic_error\n",
    "\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15781502454168053974\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5072859955\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 12167383872144736457\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Information \n",
      "\n",
      "Input Array Shape (49995, 95)\n",
      "Output Array Shape (49995, 66)\n",
      "Testing Data information \n",
      "\n",
      "Test Input Shape (7272, 95)\n",
      "Test Output Shape (7272, 66)\n"
     ]
    }
   ],
   "source": [
    "train_input_data_path = r'D:\\Users\\Vishwesh\\PycharmProjects\\shore_mapmri\\Data\\train_input_mapmri_r8.mat'\n",
    "train_output_data_path = r'D:\\Users\\Vishwesh\\PycharmProjects\\shore_mapmri\\Data\\train_output_shore.mat'\n",
    "train_input_data_path = os.path.normpath(train_input_data_path)\n",
    "train_output_data_path = os.path.normpath(train_output_data_path)\n",
    "\n",
    "test_input_data_path = r'D:\\Users\\Vishwesh\\PycharmProjects\\shore_mapmri\\Data\\test_input_mapmri_r8.mat'\n",
    "test_output_data_path = r'D:\\Users\\Vishwesh\\PycharmProjects\\shore_mapmri\\Data\\test_output_shore.mat'\n",
    "test_input_data_path = os.path.normpath(test_input_data_path)\n",
    "test_output_data_path = os.path.normpath(test_output_data_path)\n",
    "\n",
    "input = loadmat(train_input_data_path)\n",
    "output = loadmat(train_output_data_path)\n",
    "\n",
    "#X = np.array(input['train_input_shore'])\n",
    "X = np.array(input['train_input_mapmri_r8'])\n",
    "y = np.array(output['train_output_shore'])\n",
    "\n",
    "# Get dimensions of arrays\n",
    "print('Training Data Information \\n')\n",
    "x_size = X.shape\n",
    "print('Input Array Shape',x_size)\n",
    "y_size = y.shape\n",
    "print ('Output Array Shape',y_size)\n",
    "\n",
    "test_input = loadmat(test_input_data_path)\n",
    "test_output = loadmat(test_output_data_path)\n",
    "\n",
    "#X_test = np.array(test_input['test_input_shore'])\n",
    "X_test = np.array(test_input['test_input_mapmri_r8'])\n",
    "y_test = np.array(test_output['test_output_shore'])\n",
    "\n",
    "# Get dimensions of Test data\n",
    "print('Testing Data information \\n')\n",
    "x_size = X_test.shape\n",
    "print('Test Input Shape',x_size)\n",
    "y_size = y_test.shape\n",
    "print('Test Output Shape',y_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build Generator Code\n",
    "def build_generator_A(input_B,name='genA'):\n",
    "    \n",
    "    \n",
    "    y = Dense(400, activation='relu')(input_B)\n",
    "    y = Dense(66, activation='relu')(y)\n",
    "    y = Dense(200, activation='relu')(y)\n",
    "    y = Dense(95, activation='linear')(y)\n",
    "    return y\n",
    "\n",
    "    #model = Sequential()\n",
    "    # Input layer with dimension 1 and hidden layer i with 128 neurons.\n",
    "    #model.add(Dense(95, input_shape=(95,)))\n",
    "    #model.add(Dense(400))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    # Hidden layer j with 64 neurons plus activation layer.\n",
    "    #model.add(Dense(66))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    # Hidden layer k with 64 neurons.\n",
    "    #model.add(Dense(200))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    #model.add(Dense(200))\n",
    "    # Output Layer.\n",
    "    #model.add(Dense(66))\n",
    " \n",
    "    # Model is derived and compiled using mean square error as loss\n",
    "    # function, accuracy as metric and gradient descent optimizer.\n",
    "    #model.compile(loss=mean_squared_logarithmic_error, optimizer='nadam', metrics=['mse','mae'])\n",
    "    #model.summary()\n",
    "    \n",
    "\n",
    "def build_generator_B(input_A,name='genB'):\n",
    "    \n",
    "    #model = Sequential()\n",
    "    # Input layer with dimension 1 and hidden layer i with 128 neurons.\n",
    "    x = Dense(400, activation='relu')(input_A)\n",
    "    x = Dense(66, activation='relu')(x)\n",
    "    x = Dense(200, activation='relu')(x)\n",
    "    x = Dense(66, activation='linear')(x)\n",
    "    return x\n",
    "\n",
    "    #model.add(Dense(66, input_shape=(66,)))\n",
    "    #model.add(Dense(400))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    # Hidden layer j with 64 neurons plus activation layer.\n",
    "    #model.add(Dense(66))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    # Hidden layer k with 64 neurons.\n",
    "    #model.add(Dense(200))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    #model.add(Dense(200))\n",
    "    # Output Layer.\n",
    "    #model.add(Dense(95))\n",
    "    \n",
    "    \n",
    "def build_dis_A(input_B,name='disA'):\n",
    "    \n",
    "    x = Dense(400, activation='relu')(input_B)\n",
    "    x = Dense(66, activation='relu')(x)\n",
    "    x = Dense(200, activation='relu')(x)\n",
    "    x = Dense(66, activation='linear')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    return x\n",
    "    #model = Sequential()\n",
    "    # Input layer with dimension 1 and hidden layer i with 128 neurons.\n",
    "    #model.add(Dense(66, input_shape=(66,)))\n",
    "    #model.add(Dense(400))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    # Hidden layer j with 64 neurons plus activation layer.\n",
    "    #model.add(Dense(200))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    # Hidden layer k with 64 neurons.\n",
    "    #model.add(Dense(66))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    #model.add(Dense(200))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    #model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    #return model\n",
    "    \n",
    "def build_dis_B(input_A,name='disB'):\n",
    "    \n",
    "    x = Dense(400, activation='relu')(input_A)\n",
    "    x = Dense(66, activation='relu')(x)\n",
    "    x = Dense(200, activation='relu')(x)\n",
    "    x = Dense(66, activation='linear')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    return x\n",
    "    \n",
    "    #model = Sequential()\n",
    "    # Input layer with dimension 1 and hidden layer i with 128 neurons.\n",
    "    #model.add(Dense(95, input_shape=(95,)))\n",
    "    #model.add(Dense(400))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    # Hidden layer j with 64 neurons plus activation layer.\n",
    "    #model.add(Dense(200))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    # Hidden layer k with 64 neurons.\n",
    "    #model.add(Dense(66))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    #model.add(Dense(200))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    #model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    #return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "input_A = tf.placeholder(tf.float32, [batch_size, 95, 1], name=\"input_A\")\n",
    "input_B = tf.placeholder(tf.float32, [batch_size, 66, 1], name=\"input_B\")\n",
    "\n",
    "gen_B = build_generator_B(input_A, name=\"generator_AtoB\")\n",
    "gen_A = build_generator_A(input_B, name=\"generator_BtoA\")\n",
    "dec_A = build_dis_A(input_A, name=\"discriminator_A\")\n",
    "dec_B = build_dis_B(input_B, name=\"discriminator_B\")\n",
    "\n",
    "dec_gen_A = build_dis_B(gen_A, \"discriminator_A\")\n",
    "dec_gen_B = build_dis_A(gen_B, \"discriminator_B\")\n",
    "cyc_A = build_generator_B(gen_B, \"generator_BtoA\")\n",
    "cyc_B = build_generator_A(gen_A, \"generator_AtoB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_A_loss_1 = tf.reduce_mean(tf.squared_difference(dec_A,1))\n",
    "D_B_loss_1 = tf.reduce_mean(tf.squared_difference(dec_B,1))\n",
    "\n",
    "D_A_loss_2 = tf.reduce_mean(tf.square(dec_gen_A))\n",
    "D_B_loss_2 = tf.reduce_mean(tf.square(dec_gen_B))\n",
    "\n",
    "D_A_loss = (D_A_loss_1 + D_A_loss_2)/2\n",
    "D_B_loss = (D_B_loss_1 + D_B_loss_2)/2\n",
    "\n",
    "g_loss_B_1 = tf.reduce_mean(tf.squared_difference(dec_gen_A,1))\n",
    "g_loss_A_1 = tf.reduce_mean(tf.squared_difference(dec_gen_A,1))\n",
    "\n",
    "cyc_loss = tf.reduce_mean(tf.abs(input_A-cyc_A)) + tf.reduce_mean(tf.abs(input_B-cyc_B))\n",
    "\n",
    "g_loss_A = g_loss_A_1 + 10*cyc_loss\n",
    "g_loss_B = g_loss_B_1 + 10*cyc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_loss_A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7755001f7526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md_A_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md_A_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0md_B_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss_B\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md_B_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mg_A_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_loss_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mg_A_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mg_B_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_loss_B\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mg_B_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd_loss_A' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.0001)\n",
    "d_A_trainer = optimizer.minimize(d_loss_A, var_list=d_A_vars)\n",
    "d_B_trainer = optimizer.minimize(d_loss_B, var_list=d_B_vars)\n",
    "g_A_trainer = optimizer.minimize(g_loss_A, var_list=g_A_vars)\n",
    "g_B_trainer = optimizer.minimize(g_loss_B, var_list=g_B_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_l",
   "language": "python",
   "name": "deep_l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
