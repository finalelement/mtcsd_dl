{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat, savemat\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from keras.optimizers import SGD, adam, nadam, Adagrad\n",
    "from keras.regularizers import l1,l2\n",
    "\n",
    "from keras.callbacks import EarlyStopping, CSVLogger\n",
    "from keras.losses import mean_squared_logarithmic_error\n",
    "\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 18279197345194029353\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 104837939\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 530339961367822436\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc_loss(y_true, y_pred):\n",
    "    \n",
    "    # Subtract mean value from expansions of true and pred\n",
    "    x_true = y_true[:,1:66]\n",
    "    x_pred = y_pred[:,1:66]\n",
    "    \n",
    "    # Normalize each vector\n",
    "    comp_true = tf.conj(x_true)\n",
    "    norm_true = x_true / tf.sqrt(tf.reduce_sum(tf.multiply(x_true,comp_true)))\n",
    "    \n",
    "    comp_pred = tf.conj(x_pred)\n",
    "    norm_pred = x_pred / tf.sqrt(tf.reduce_sum(tf.multiply(x_pred,comp_pred)))\n",
    "    \n",
    "    comp_p2 = tf.conj(norm_pred)\n",
    "    acc = tf.real(tf.reduce_sum(tf.multiply(norm_true,comp_p2)))\n",
    "    acc = -1.0 * acc * acc\n",
    "    \n",
    "    loss_mse = K.mean(K.square(y_pred-y_true))\n",
    "    \n",
    "    return acc + loss_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_all(input_path,output_path,test_input_path,test_output_path):\n",
    "    \n",
    "    input = loadmat(input_path)\n",
    "    output = loadmat(output_path)\n",
    "    \n",
    "    #X = np.array(input['train_input_shore'])\n",
    "    X = np.array(input['train_shore_input'])\n",
    "    y = np.array(output['train_shore_output'])\n",
    "    \n",
    "    # Get dimensions of arrays\n",
    "    print('Training Data Information \\n')\n",
    "    x_size = X.shape\n",
    "    print('Input Array Shape',x_size)\n",
    "    y_size = y.shape\n",
    "    print ('Output Array Shape',y_size)\n",
    "    \n",
    "    test_input = loadmat(test_input_path)\n",
    "    test_output = loadmat(test_output_path)\n",
    "    \n",
    "    #X_test = np.array(test_input['test_input_shore'])\n",
    "    X_test = np.array(test_input['test_shore_input'])\n",
    "    y_test = np.array(test_output['test_shore_output'])\n",
    "    \n",
    "    # Get dimensions of Test data\n",
    "    print('Testing Data information \\n')\n",
    "    x_size = X_test.shape\n",
    "    print('Test Input Shape',x_size)\n",
    "    y_size = y_test.shape\n",
    "    print('Test Output Shape',y_size)\n",
    "    \n",
    "    return X,y,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn2():\n",
    "    model = Sequential()\n",
    "    # Input layer with dimension 1 and hidden layer i with 128 neurons.\n",
    "    model.add(Dense(50, input_shape=(50,)))\n",
    "    model.add(Dense(400))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(Dropout(0.6))\n",
    "    # Hidden layer j with 64 neurons plus activation layer.\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(Dropout(0.5))\n",
    "    # Hidden layer k with 64 neurons.\n",
    "    model.add(Dense(66))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(400))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    # Output Layer.\n",
    "    model.add(Dense(50))\n",
    " \n",
    "    # Model is derived and compiled using mean square error as loss\n",
    "    # function, accuracy as metric and gradient descent optimizer.\n",
    "    model.compile(loss='mse', optimizer='RMSProp', metrics=['mse','mae'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Originally number of epochs was set to 1000, currently at 10.\n",
    "def train_nn(model, X, y, out_dir, val_size=0.1, n_epoch=1000):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    csv_logger = CSVLogger(os.path.join(out_dir, 'results.csv'))\n",
    "\n",
    "    model.fit(X, y, epochs=n_epoch, batch_size=1000, verbose=1, shuffle=True, validation_split=val_size, callbacks=[csv_logger])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_estimate(model, X, y, out_file, indices):\n",
    "    #y_pred = model.predict(X)\n",
    "\n",
    "    #y_pred = y_scaler.inverse_transform(y_pred)\n",
    "    #y = y_scaler.inverse_transform(y)\n",
    "\n",
    "    out_path = os.path.dirname(out_file)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "     \n",
    "    y_pred = model.predict(X)\n",
    "    savemat(out_file, mdict={'out_pred': y_pred, 'out_true': y, 'indices': indices})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_test_set_prediction(model, out_file, X_test, y_test):\n",
    "    \n",
    "    # Get dimensions of arrays\n",
    "    x_size = X_test.shape\n",
    "    print('Hist 72: Input Array Shape',x_size)\n",
    "    y_size = y_test.shape\n",
    "    print ('Output Hist 72: Array Shape',y_size)\n",
    "    \n",
    "    # Make Predictions\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    # If output path does not exist, create it\n",
    "    out_path = os.path.dirname(out_file)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    \n",
    "    savemat(out_file, mdict={'out_pred': pred, 'out_true': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vishabyte_predictions(model, out_file):\n",
    "    input_file_path = r'D:\\Users\\Vishwesh\\PycharmProjects\\shore_mapmri\\Data\\vish_feed.mat'\n",
    "    input_file_path = os.path.normpath(input_file_path)\n",
    "    input_test = loadmat(input_file_path)\n",
    "    X_f_t = np.array(input_test['vish_feed'])\n",
    "    # Get dimensions of arrays\n",
    "    x_size = X_f_t.shape\n",
    "    print('Vishabyte: Input Array Shape',x_size)\n",
    "    pred = model.predict(X_f_t)\n",
    "    \n",
    "    # If output path does not exist, create it\n",
    "    out_path = os.path.dirname(out_file)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "        \n",
    "    # In vivo save matrix\n",
    "    savemat(out_file, mdict={'out_pred': pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    #args = parse_args()\n",
    "    work_dir = r'D:\\Users\\Vishwesh\\PycharmProjects\\shore_mapmri\\dl_results'\n",
    "    word_dir = os.path.normpath(work_dir)\n",
    "    \n",
    "    exp = 'Shore_input_Shore_output_v1'\n",
    "    itr = 4 # Estimated from CV\n",
    "\n",
    "    train_input_data_path = r'D:\\Users\\Vishwesh\\PycharmProjects\\shore_mapmri\\Data\\train_input_shore_v3.mat'\n",
    "    train_output_data_path = r'D:\\Users\\Vishwesh\\PycharmProjects\\shore_mapmri\\Data\\train_output_shore_v3.mat'\n",
    "    train_input_data_path = os.path.normpath(train_input_data_path)\n",
    "    train_output_data_path = os.path.normpath(train_output_data_path)\n",
    "    \n",
    "    test_input_data_path = r'D:\\Users\\Vishwesh\\PycharmProjects\\shore_mapmri\\Data\\test_input_shore_v3.mat'\n",
    "    test_output_data_path = r'D:\\Users\\Vishwesh\\PycharmProjects\\shore_mapmri\\Data\\test_output_shore_v3.mat'\n",
    "    test_input_data_path = os.path.normpath(test_input_data_path)\n",
    "    test_output_data_path = os.path.normpath(test_output_data_path)\n",
    "    \n",
    "    print (\"Loading data\")\n",
    "    X, y, X_b_test, y_b_test = load_all(train_input_data_path,train_output_data_path,test_input_data_path,test_output_data_path)\n",
    "    indices = np.array(range(X.shape[0]))+1\n",
    "\n",
    "    out_start_dir = os.path.join(work_dir,exp)\n",
    "    if not os.path.exists(out_start_dir):\n",
    "        os.makedirs(out_start_dir)\n",
    "\n",
    "    seed1 = 46\n",
    "    seed2 = 23\n",
    "    \n",
    "    kf = KFold(n_splits=5, random_state=seed1,shuffle=True,)\n",
    "    \n",
    "    fold_num = 0\n",
    "    model_D = build_nn2()\n",
    "    \n",
    "    \n",
    "    for train, test in kf.split(X):\n",
    "        # Set up training / testing data\n",
    "        fold_num += 1\n",
    "        X_train = X[train,:]\n",
    "        y_train = y[train,:]\n",
    "        X_test = X[test,:]\n",
    "        y_test = y[test,:]\n",
    "        indices_train = indices[train]\n",
    "        indices_test = indices[test]\n",
    "        #X_train, X_test, y_train, y_test, X_scaler, y_scaler = norm_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "\n",
    "        # Want to submit this w/ 1000 different initializations\n",
    "        np.random.seed(seed=seed2)\n",
    "\n",
    "        # Deep NN\n",
    "        print (\"Training DNN with %d iterations, fold %d\" % (itr, fold_num))\n",
    "        out_dir_DNN = os.path.join(out_start_dir, str(fold_num))\n",
    "        model_D = train_nn(model_D, X_train, y_train, out_dir_DNN, n_epoch=itr, val_size=0.1)\n",
    "\n",
    "        print (\"Saving training outputs\")\n",
    "        end_dir = os.path.join(out_start_dir, str(fold_num), 'training.csv')\n",
    "        save_estimate(model_D, X_train, y_train, end_dir,indices_train)\n",
    "\n",
    "        print (\"Saving testing outputs\")\n",
    "        end_dir = os.path.join(out_start_dir, str(fold_num), 'testing.csv')\n",
    "        save_estimate(model_D, X_test, y_test, end_dir,indices_test)\n",
    "\n",
    "    #print (\"Saving Vishabyte outputs\")\n",
    "    #end_dir = os.path.join(out_start_dir, str('Vishabyte_Test'), 'result_vol_invivo_vish_b2000.mat')\n",
    "    #save_vishabyte_predictions(model_D, end_dir)\n",
    "\n",
    "    print (\"Saving Histology blind test outputs\")\n",
    "    end_dir = os.path.join(out_start_dir, str('Hist_Blind_72_Test'), 'result.mat')\n",
    "    save_test_set_prediction(model_D, end_dir, X_b_test, y_b_test)\n",
    "    \n",
    "    print (\"Saving Vishabyte Predictions\")\n",
    "    end_dir = os.path.join(out_start_dir, str('Vishabyte_Preds'), 'result.mat')\n",
    "    save_vishabyte_predictions(model_D, end_dir)\n",
    "                                   \n",
    "    '''\n",
    "    # Deep NN\n",
    "    print (\"Training DNN with %d iterations\" % (itr))\n",
    "    out_dir_DNN = os.path.join(out_start_dir, 'training_results')\n",
    "    model_D = train_nn(model_D, X, y, out_dir_DNN, n_epoch=itr, val_size=0.2)\n",
    "\n",
    "    print (\"Saving testing outputs\")\n",
    "    end_dir = os.path.join(out_start_dir, 'testing_results')\n",
    "    if not os.path.exists(end_dir):\n",
    "        os.makedirs(end_dir)                           \n",
    "    save_estimate(model_D, X_test, y_test, end_dir)\n",
    "\n",
    "    kf = KFold(n_splits=5, random_state=seed1,shuffle=True,)\n",
    "\n",
    "    fold_num = 0\n",
    "    \n",
    "    \n",
    "    for train, test in kf.split(X):\n",
    "        # Set up training / testing data\n",
    "        fold_num += 1\n",
    "        X_train = X[train,:]\n",
    "        y_train = y[train,:]\n",
    "        X_test = X[test,:]\n",
    "        y_test = y[test,:]\n",
    "        indices_train = indices[train]\n",
    "        indices_test = indices[test]\n",
    "        #X_train, X_test, y_train, y_test, X_scaler, y_scaler = norm_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        # Want to submit this w/ 1000 different initializations\n",
    "        np.random.seed(seed=seed2)\n",
    "\n",
    "        # Deep NN\n",
    "        print (\"Training DNN with %d iterations, fold %d\" % (itr, fold_num))\n",
    "        out_dir_DNN = os.path.join(out_start_dir, str(fold_num))\n",
    "        model_D = train_nn(model_D, X_train, y_train, out_dir_DNN, n_epoch=itr, val_size=0.2)\n",
    "\n",
    "        print (\"Saving training outputs\")\n",
    "        end_dir = os.path.join(out_start_dir, str(fold_num), 'training.csv')\n",
    "        save_estimate(model_D, X_train, y_train, end_dir,indices_train)\n",
    "\n",
    "        print (\"Saving testing outputs\")\n",
    "        end_dir = os.path.join(out_start_dir, str(fold_num), 'testing.csv')\n",
    "        save_estimate(model_D, X_test, y_test, end_dir,indices_test)\n",
    "\n",
    "    #print (\"Saving Vishabyte outputs\")\n",
    "    #end_dir = os.path.join(out_start_dir, str('Vishabyte_Test'), 'result_vol_invivo_vish_b2000.mat')\n",
    "    #save_vishabyte_predictions(model_D, end_dir)\n",
    "\n",
    "    print (\"Saving Histology blind test outputs\")\n",
    "    end_dir = os.path.join(out_start_dir, str('Hist_Blind_72_Test'), 'result_b6000_testing_10th_order.mat')\n",
    "    save_test_set_prediction(model_D, end_dir)\n",
    "\n",
    "    #print (\"Saving Pair Wise Reprod Results\")\n",
    "    #end_dir_1 = os.path.join(out_start_dir, str('ISMRM_TS01_Test'), 'result_scan_a.mat')\n",
    "    #end_dir_2 = os.path.join(out_start_dir, str('ISMRM_TS01_Test'), 'result_scan_b.mat')\n",
    "    #pair_wise_reprod_check_2(model_D, end_dir_1, end_dir_2)\n",
    "    \n",
    "    print (\"Saving Pair Wise Reprod Results on HCP\")\n",
    "    end_dir_1 = os.path.join(out_start_dir, str('HCP_Test'), 'result_scan_a.mat')\n",
    "    end_dir_2 = os.path.join(out_start_dir, str('HCP_Test'), 'result_scan_b.mat')\n",
    "    pair_wise_reprod_check_3(model_D, end_dir_1, end_dir_2)\n",
    "    \n",
    "    print (\"Saving network for this fold\")\n",
    "    net_dir_DNN = (os.path.join(out_start_dir, 'nets'))\n",
    "    if not os.path.exists(net_dir_DNN):\n",
    "        os.makedirs(net_dir_DNN)\n",
    "    net_name = os.path.join(net_dir_DNN, exp+\"_model_fold\"+str(fold_num)+\".h5\")\n",
    "    model_D.save(net_name)\n",
    "\n",
    "    t1 = time.time()\n",
    "    total_time = t1-t0\n",
    "    print (total_time)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Training Data Information \n",
      "\n",
      "Input Array Shape (49995, 50)\n",
      "Output Array Shape (49995, 50)\n",
      "Testing Data information \n",
      "\n",
      "Test Input Shape (7272, 50)\n",
      "Test Output Shape (7272, 50)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               20400     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 66)                13266     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 66)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               13400     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 400)               80400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                20050     \n",
      "=================================================================\n",
      "Total params: 230,266\n",
      "Trainable params: 230,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training DNN with 4 iterations, fold 1\n",
      "Train on 35996 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "35996/35996 [==============================] - 4s - loss: 864.7663 - mean_squared_error: 864.7663 - mean_absolute_error: 18.6478 - val_loss: 837.1668 - val_mean_squared_error: 837.1669 - val_mean_absolute_error: 17.1266\n",
      "Epoch 2/4\n",
      "35996/35996 [==============================] - 0s - loss: 653.5143 - mean_squared_error: 653.5144 - mean_absolute_error: 16.0230 - val_loss: 848.6035 - val_mean_squared_error: 848.6036 - val_mean_absolute_error: 18.0954\n",
      "Epoch 3/4\n",
      "35996/35996 [==============================] - 0s - loss: 640.9976 - mean_squared_error: 640.9977 - mean_absolute_error: 15.8178 - val_loss: 760.8586 - val_mean_squared_error: 760.8588 - val_mean_absolute_error: 17.3624\n",
      "Epoch 4/4\n",
      "35996/35996 [==============================] - 0s - loss: 585.3473 - mean_squared_error: 585.3473 - mean_absolute_error: 15.0002 - val_loss: 674.5782 - val_mean_squared_error: 674.5782 - val_mean_absolute_error: 15.8913\n",
      "Saving training outputs\n",
      "Saving testing outputs\n",
      "Training DNN with 4 iterations, fold 2\n",
      "Train on 35996 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "35996/35996 [==============================] - 0s - loss: 537.9849 - mean_squared_error: 537.9850 - mean_absolute_error: 14.3206 - val_loss: 665.1655 - val_mean_squared_error: 665.1656 - val_mean_absolute_error: 15.9792\n",
      "Epoch 2/4\n",
      "35996/35996 [==============================] - 0s - loss: 476.2710 - mean_squared_error: 476.2710 - mean_absolute_error: 13.3511 - val_loss: 625.6249 - val_mean_squared_error: 625.6249 - val_mean_absolute_error: 15.1501\n",
      "Epoch 3/4\n",
      "35996/35996 [==============================] - 0s - loss: 435.1312 - mean_squared_error: 435.1313 - mean_absolute_error: 12.6789 - val_loss: 550.4335 - val_mean_squared_error: 550.4335 - val_mean_absolute_error: 14.1630\n",
      "Epoch 4/4\n",
      "35996/35996 [==============================] - 0s - loss: 400.5591 - mean_squared_error: 400.5592 - mean_absolute_error: 12.1269 - val_loss: 512.3958 - val_mean_squared_error: 512.3958 - val_mean_absolute_error: 13.2443\n",
      "Saving training outputs\n",
      "Saving testing outputs\n",
      "Training DNN with 4 iterations, fold 3\n",
      "Train on 35996 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "35996/35996 [==============================] - 0s - loss: 392.4781 - mean_squared_error: 392.4781 - mean_absolute_error: 11.8894 - val_loss: 466.0948 - val_mean_squared_error: 466.0948 - val_mean_absolute_error: 12.7780\n",
      "Epoch 2/4\n",
      "35996/35996 [==============================] - 0s - loss: 366.2442 - mean_squared_error: 366.2443 - mean_absolute_error: 11.5318 - val_loss: 467.1376 - val_mean_squared_error: 467.1376 - val_mean_absolute_error: 13.0010\n",
      "Epoch 3/4\n",
      "35996/35996 [==============================] - 0s - loss: 357.6660 - mean_squared_error: 357.6660 - mean_absolute_error: 11.3541 - val_loss: 470.6672 - val_mean_squared_error: 470.6673 - val_mean_absolute_error: 12.6391\n",
      "Epoch 4/4\n",
      "35996/35996 [==============================] - 0s - loss: 344.1521 - mean_squared_error: 344.1521 - mean_absolute_error: 11.1936 - val_loss: 550.4801 - val_mean_squared_error: 550.4802 - val_mean_absolute_error: 12.8543\n",
      "Saving training outputs\n",
      "Saving testing outputs\n",
      "Training DNN with 4 iterations, fold 4\n",
      "Train on 35996 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "35996/35996 [==============================] - 0s - loss: 338.9110 - mean_squared_error: 338.9110 - mean_absolute_error: 11.0547 - val_loss: 477.0833 - val_mean_squared_error: 477.0833 - val_mean_absolute_error: 12.7129\n",
      "Epoch 2/4\n",
      "35996/35996 [==============================] - 0s - loss: 328.4007 - mean_squared_error: 328.4008 - mean_absolute_error: 10.9558 - val_loss: 506.6388 - val_mean_squared_error: 506.6388 - val_mean_absolute_error: 12.7402\n",
      "Epoch 3/4\n",
      "35996/35996 [==============================] - 0s - loss: 319.3178 - mean_squared_error: 319.3178 - mean_absolute_error: 10.8037 - val_loss: 475.7643 - val_mean_squared_error: 475.7643 - val_mean_absolute_error: 12.5515\n",
      "Epoch 4/4\n",
      "35996/35996 [==============================] - 0s - loss: 313.8406 - mean_squared_error: 313.8406 - mean_absolute_error: 10.7665 - val_loss: 471.7733 - val_mean_squared_error: 471.7734 - val_mean_absolute_error: 12.5635\n",
      "Saving training outputs\n",
      "Saving testing outputs\n",
      "Training DNN with 4 iterations, fold 5\n",
      "Train on 35996 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "35996/35996 [==============================] - 0s - loss: 305.5480 - mean_squared_error: 305.5480 - mean_absolute_error: 10.6291 - val_loss: 547.2008 - val_mean_squared_error: 547.2009 - val_mean_absolute_error: 13.2574\n",
      "Epoch 2/4\n",
      "35996/35996 [==============================] - 0s - loss: 300.8442 - mean_squared_error: 300.8443 - mean_absolute_error: 10.5512 - val_loss: 565.7777 - val_mean_squared_error: 565.7778 - val_mean_absolute_error: 13.3688\n",
      "Epoch 3/4\n",
      "35996/35996 [==============================] - 0s - loss: 294.7644 - mean_squared_error: 294.7644 - mean_absolute_error: 10.4662 - val_loss: 472.4972 - val_mean_squared_error: 472.4972 - val_mean_absolute_error: 12.4087\n",
      "Epoch 4/4\n",
      "35996/35996 [==============================] - 0s - loss: 287.4657 - mean_squared_error: 287.4657 - mean_absolute_error: 10.3641 - val_loss: 491.1806 - val_mean_squared_error: 491.1807 - val_mean_absolute_error: 12.5438\n",
      "Saving training outputs\n",
      "Saving testing outputs\n",
      "Saving Histology blind test outputs\n",
      "Hist 72: Input Array Shape (7272, 50)\n",
      "Output Hist 72: Array Shape (7272, 50)\n",
      "Saving Vishabyte Predictions\n",
      "Vishabyte: Input Array Shape (544050, 50)\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_l",
   "language": "python",
   "name": "deep_l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
